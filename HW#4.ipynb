{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f30084-401f-4708-90f1-c60e9927f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "заметки:\n",
    "- baseline нужен только как стартовая точка\n",
    "- full обычно дает максимум качества, но дольше\n",
    "- linear probing быстрый, но часто чуть хуже, посмотрим\n",
    "- LoRA — компромисс по времени/качеству, тоже сравним метрики\n",
    "\n",
    "Соревнование: https://huggingface.co/datasets/AlexSham/Toxic_Russian_Comments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc3fda-16b9-48b7-b797-94f8df56414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Обоснования:\n",
    "- `ruBert-base` — адекватная база под RU, но тяжелая, возьмем tiny модельку, быстрее и с сохранением приемлемого качества\n",
    "- метрики: accuracy + f1/precision/recall для дисбаланса\n",
    "- для full и LoRA — сделаем маленький LR, большее тщательно обучаются; для линейной головы — lernin rate можно побольше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086dd554-7056-4ea3-8837-00951885f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.1.1\n",
      "    Uninstalling accelerate-1.1.1:\n",
      "      Successfully uninstalled accelerate-1.1.1\n",
      "Successfully installed accelerate-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ffi (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ffi (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ffi (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (C:\\Users\\user\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoawq 0.2.6 requires torch==2.3.1, but you have torch 2.7.1+cu118 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8bc580-0933-41b4-b573-009872d2ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# сначала подтянем либы и зафиксируем сиды, чтобы все воспроизводилось\n",
    "# база\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import ClassLabel\n",
    "\n",
    "# датасеты + трансформеры (токенизация и тренер)\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    TrainingArguments # непонятная ошибка в окружении, должно помочь\n",
    ")\n",
    "\n",
    "# метрики для классификации\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# LoRA\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# сиды — чтобы результаты были стабильнее\n",
    "SEED = 2003\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# девайс и fp16, если доступно\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "print(\"device:\", DEVICE)\n",
    "\n",
    "# базовая модель под русский || простую руберт моя gpu не потянула бы, а в google collab потерял лимиты(\n",
    "MODEL_NAME = \"cointegrated/rubert-tiny2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49d5c77d-b266-4703-8c9f-ecffa3296595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.820351\n",
      "1    0.179649\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.abspath(\".\")\n",
    "train_path = os.path.join(data_dir, \"train.jsonl\")\n",
    "test_path = os.path.join(data_dir, \"test.jsonl\")\n",
    "\n",
    "raw = load_dataset(\"json\", data_files={\"train\": train_path, \"test\": test_path})\n",
    "\n",
    "# метки классов\n",
    "labels = raw[\"train\"].unique(\"label\")\n",
    "labels = sorted(labels)  # желательно отсортировать для консистентности\n",
    "\n",
    "# ClassLabel с этими метками\n",
    "class_label = ClassLabel(names=labels)\n",
    "\n",
    "# cast, чтобы заменить тип колонки label на ClassLabel\n",
    "raw[\"train\"] = raw[\"train\"].cast_column(\"label\", class_label)\n",
    "\n",
    "# стратифицированный сплит\n",
    "split = raw[\"train\"].train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"label\",\n",
    ")\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "test_ds = raw[\"test\"]\n",
    "\n",
    "# баланс???\n",
    "print(pd.Series(train_ds[\"label\"]).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15b56f4-294d-4b6b-8e1c-9354aa063757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc244b8e36da4d3bba343946aa5d5e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--cointegrated--rubert-tiny2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66fe81dcde948e9a37ad6c3e4dd6957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c654cd6f1544c7592fcbe5e7d1a1fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b69867787ef4186bae3e6420d20794b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e8170ccbd74ab6851b7e6062bfd745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b27bb004c94f879e6046e071103b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeb9f289a184db89e8e2f3594dcedca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(0), 'input_ids': tensor([    2,   312, 26629,  1619, 11352, 10164,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# токенизируем тексты, дальше работаем уже с тензорами\n",
    "# токенизатор под выбранную модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "max_length = 160\n",
    "\n",
    "# функция токенизации батча\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "# прогоняем все сплиты\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "test_tok = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# динамическая подгонка паддинга в батче\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# приводим к torch-формату\n",
    "train_tok.set_format(\"torch\")\n",
    "val_tok.set_format(\"torch\")\n",
    "test_tok.set_format(\"torch\")\n",
    "\n",
    "# смотрим, что получилось\n",
    "print(train_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c938a23-5922-4afd-8be8-2bff40a5851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь метрики и фабрика тренера, чтобы не копипастить\n",
    "# набор метрик для eval\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "    }\n",
    "\n",
    "\n",
    "# общий конструктор тренера под разные режимы\n",
    "def build_trainer(model, lr, output_dir, num_epochs=3):\n",
    "    # тут все базовые гиперы, чтобы было одинаково в сравнениях\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=500,  # реже логгируем чтоб быстрее обучиться\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "        seed=SEED,\n",
    "        dataloader_num_workers=4,\n",
    "    )\n",
    "\n",
    "\n",
    "    # сам тренер\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "# отдельный eval без обучения — для baseline\n",
    "def eval_only(model, eval_dataset, tokenizer, collator, compute_metrics):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"tmp_eval\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        report_to=\"none\",\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7f488e-778a-4284-9be4-032f600d890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df33bd7dff174cb798efaf7703d17eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4404569b7a46aaafa5ce7dd8659a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/118M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22116\\4093402090.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1397' max='1397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1397/1397 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6966676115989685, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.46444712936859534, 'eval_f1': 0.25498007968127495, 'eval_precision': 0.16997261183500706, 'eval_recall': 0.5100871731008717, 'eval_runtime': 26.5633, 'eval_samples_per_second': 841.272, 'eval_steps_per_second': 52.591}\n"
     ]
    }
   ],
   "source": [
    "# baseline без обучения — просто чтобы видеть стартовую точку\n",
    "# загружаем модель с классификационной головой\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    num_labels=2,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "baseline_metrics = eval_only(\n",
    "    baseline_model,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf8b211-eec0-4de2-85f6-aa98aa9412a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22116\\4093402090.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25140' max='25140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25140/25140 25:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.116595</td>\n",
       "      <td>0.971495</td>\n",
       "      <td>0.920820</td>\n",
       "      <td>0.919107</td>\n",
       "      <td>0.922540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.116595059633255,\n",
       " 'eval_accuracy': 0.9714950552646888,\n",
       " 'eval_f1': 0.9208203853325047,\n",
       " 'eval_precision': 0.919106699751861,\n",
       " 'eval_recall': 0.9225404732254048,\n",
       " 'eval_runtime': 129.2843,\n",
       " 'eval_samples_per_second': 172.852,\n",
       " 'eval_steps_per_second': 21.611,\n",
       " 'epoch': 1.0,\n",
       " 'train_time_sec': 1634.379587650299}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full finetuning — трогаем все веса\n",
    "# свежая модель под обучение\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# тренер для полного дообучения\n",
    "full_trainer = build_trainer(full_model, lr=2e-5, output_dir=\"runs/full\")\n",
    "\n",
    "# меряем время обучения\n",
    "start = time.time()\n",
    "full_trainer.train()\n",
    "full_time = time.time() - start\n",
    "\n",
    "# финальные метрики\n",
    "full_metrics = full_trainer.evaluate()\n",
    "full_metrics[\"train_time_sec\"] = full_time\n",
    "full_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308a8409-7357-49e9-bd7e-b3addf258f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22116\\4093402090.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25140' max='25140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25140/25140 14:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.264719</td>\n",
       "      <td>0.888576</td>\n",
       "      <td>0.646307</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.566625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2794' max='2794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2794/2794 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26471850275993347,\n",
       " 'eval_accuracy': 0.8885756477379514,\n",
       " 'eval_f1': 0.6463068181818183,\n",
       " 'eval_precision': 0.7520661157024794,\n",
       " 'eval_recall': 0.5666251556662516,\n",
       " 'eval_runtime': 129.134,\n",
       " 'eval_samples_per_second': 173.053,\n",
       " 'eval_steps_per_second': 21.636,\n",
       " 'epoch': 1.0,\n",
       " 'train_time_sec': 949.4691407680511}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear probing — база заморожена, учим только голову\n",
    "# загружаем модель\n",
    "lp_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# замораживаем все кроме классификатора\n",
    "for p in lp_model.base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# тренер с повышенным LR (обучается только голова)\n",
    "lp_trainer = build_trainer(lp_model, lr=5e-4, output_dir=\"runs/linear_probing\")\n",
    "\n",
    "# время и обучение\n",
    "start = time.time()\n",
    "lp_trainer.train()\n",
    "lp_time = time.time() - start\n",
    "\n",
    "# финальная оценка\n",
    "lp_metrics = lp_trainer.evaluate()\n",
    "lp_metrics[\"train_time_sec\"] = lp_time\n",
    "lp_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020b706-78ed-4e03-86b5-524e4fb3b204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b215f19-49eb-4a6e-ae93-4ed64083e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'shard_checkpoint' from 'transformers.modeling_utils' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# конфиг: какие слои и насколько сильно крутим\u001b[39;00m\n\u001b[0;32m      8\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m      9\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_CLS,\n\u001b[0;32m     10\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m get_peft_model(lora_model, lora_config)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# LR чуть больше, чем в full — тут обучаем мало параметров\u001b[39;00m\n\u001b[0;32m     20\u001b[0m lora_trainer \u001b[38;5;241m=\u001b[39m build_trainer(lora_model, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/lora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\mapping_func.py:122\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftModel(\n\u001b[0;32m    115\u001b[0m         model,\n\u001b[0;32m    116\u001b[0m         peft_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[0;32m    123\u001b[0m     model,\n\u001b[0;32m    124\u001b[0m     peft_config,\n\u001b[0;32m    125\u001b[0m     adapter_name\u001b[38;5;241m=\u001b[39madapter_name,\n\u001b[0;32m    126\u001b[0m     autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype,\n\u001b[0;32m    127\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\peft_model.py:1656\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.__init__\u001b[1;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m         peft_config\u001b[38;5;241m.\u001b[39mmodules_to_save\u001b[38;5;241m.\u001b[39mextend(classifier_module_names)\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;66;03m# The modification of peft_config must happen before the init call as the `modules_to_save` information\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;66;03m# will be used to guard the target layer matching against matching `modules_to_save` layers. Only the\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# config is relevant for this, the `modules_to_save` attribute can follow later.\u001b[39;00m\n\u001b[1;32m-> 1656\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, peft_config, adapter_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules_to_save\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mnamed_children():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\peft_model.py:129\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    127\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[1;32m--> 129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, {adapter_name: peft_config}, adapter_name)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_cast_adapter_dtype(\n\u001b[0;32m    133\u001b[0m         adapter_name\u001b[38;5;241m=\u001b[39madapter_name, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype\n\u001b[0;32m    134\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:298\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minject_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, adapter_name, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage, state_dict\u001b[38;5;241m=\u001b[39mstate_dict)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:804\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[0;32m    802\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[1;32m--> 804\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(\n\u001b[0;32m    805\u001b[0m                 peft_config, adapter_name, target, target_name, parent, current_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m    806\u001b[0m             )\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;66;03m# use the state_dict to match modules instead\u001b[39;00m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module_names:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:250\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[1;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, parameter_name)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to target the same nn.Parameter twice, this should not happen. Please open an issue on the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEFT repo: https://github.com/huggingface/peft/issues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    249\u001b[0m device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhf_device_map \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_module(lora_config, adapter_name, target, device_map\u001b[38;5;241m=\u001b[39mdevice_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:337\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[1;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dispatcher \u001b[38;5;129;01min\u001b[39;00m dispatchers:\n\u001b[1;32m--> 337\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m dispatcher(target, adapter_name, lora_config\u001b[38;5;241m=\u001b[39mlora_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# first match wins\u001b[39;00m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\awq.py:105\u001b[0m, in \u001b[0;36mdispatch_awq\u001b[1;34m(target, adapter_name, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m     target_base_layer \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_auto_awq_available():\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WQLinear_GEMM\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_base_layer, WQLinear_GEMM):\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;66;03m# Raise the error only at the dispatch level\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         AUTOAWQ_MINIMUM_VERSION \u001b[38;5;241m=\u001b[39m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\awq\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoAWQForCausalLM\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\awq\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MptAWQForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LlamaAWQForCausalLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptAWQForCausalLM\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\awq\\models\\mpt.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseAWQForCausalLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_mpt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MptBlock \u001b[38;5;28;01mas\u001b[39;00m OldMptBlock, MptForCausalLM\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMptAWQForCausalLM\u001b[39;00m(BaseAWQForCausalLM):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\awq\\models\\base.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc, Annotated\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shard_checkpoint\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     WQLinear_GEMM,\n\u001b[0;32m     18\u001b[0m     WQLinear_GEMV,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     qbits_post_init,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     get_named_linears,\n\u001b[0;32m     31\u001b[0m     set_op_by_name,\n\u001b[0;32m     32\u001b[0m     exclude_layers_to_not_quantize,\n\u001b[0;32m     33\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'shard_checkpoint' from 'transformers.modeling_utils' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py)"
     ]
    }
   ],
   "source": [
    "# LoRA — быстрый вариант: учим адаптеры, не все веса\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# конфиг: какие слои и насколько сильно крутим\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"],\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "# LR чуть больше, чем в full — тут обучаем мало параметров\n",
    "lora_trainer = build_trainer(lora_model, lr=2e-4, output_dir=\"runs/lora\")\n",
    "\n",
    "start = time.time()\n",
    "lora_trainer.train()\n",
    "lora_time = time.time() - start\n",
    "\n",
    "lora_metrics = lora_trainer.evaluate()\n",
    "lora_metrics[\"train_time_sec\"] = lora_time\n",
    "lora_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592678bf-19f6-4635-8627-8a07410810a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# По какой-то причине не получилось лору использовать, несовместимость библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ab5197-f3f6-4d86-9c25-0af710132276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.6\n",
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import peft\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d728299-486b-45dd-97a2-eb776deaa4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>train_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full</td>\n",
       "      <td>0.971495</td>\n",
       "      <td>0.920820</td>\n",
       "      <td>0.919107</td>\n",
       "      <td>0.922540</td>\n",
       "      <td>1634.379588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.888576</td>\n",
       "      <td>0.646307</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.566625</td>\n",
       "      <td>949.469141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.464447</td>\n",
       "      <td>0.254980</td>\n",
       "      <td>0.169973</td>\n",
       "      <td>0.510087</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  accuracy        f1  precision    recall  train_time_sec\n",
       "1            full  0.971495  0.920820   0.919107  0.922540     1634.379588\n",
       "2  linear_probing  0.888576  0.646307   0.752066  0.566625      949.469141\n",
       "0        baseline  0.464447  0.254980   0.169973  0.510087             NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сводим метрики в табличку для честного сравнения\n",
    "# helper, чтобы не писать одно и то же\n",
    "def metrics_to_row(name, m):\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": m.get(\"eval_accuracy\"),\n",
    "        \"f1\": m.get(\"eval_f1\"),\n",
    "        \"precision\": m.get(\"eval_precision\"),\n",
    "        \"recall\": m.get(\"eval_recall\"),\n",
    "        \"train_time_sec\": m.get(\"train_time_sec\"),\n",
    "    }\n",
    "# собираем все результаты в один список\n",
    "rows = [\n",
    "    metrics_to_row(\"baseline\", {\"eval_accuracy\": baseline_metrics[\"eval_accuracy\"],\n",
    "                                \"eval_f1\": baseline_metrics[\"eval_f1\"],\n",
    "                                \"eval_precision\": baseline_metrics[\"eval_precision\"],\n",
    "                                \"eval_recall\": baseline_metrics[\"eval_recall\"],\n",
    "                                \"train_time_sec\": None}),\n",
    "    metrics_to_row(\"full\", full_metrics),\n",
    "    metrics_to_row(\"linear_probing\", lp_metrics)\n",
    "    # metrics_to_row(\"lora\", lora_metrics),\n",
    "]\n",
    "# сортируем по f1\n",
    "pd.DataFrame(rows).sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e5733-e459-4cec-83db-b3d32d6fad14",
   "metadata": {},
   "source": [
    "full модель показывает лучший результат. Особенно сильно в лучшую сторону отличаются precision and recall. \n",
    "baseline - случайный результат, что естественно\n",
    "lora - не получилось обучить из-за проблем бесконечных с библиотеками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aea2011-1657-4cd4-928a-0ee6de8daa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.252617</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.855211</td>\n",
       "      <td>21500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2712</td>\n",
       "      <td>0.485519</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>22000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2886</td>\n",
       "      <td>0.259065</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>22500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2799</td>\n",
       "      <td>0.450012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.914877</td>\n",
       "      <td>23000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.483303</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.934765</td>\n",
       "      <td>23500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2850</td>\n",
       "      <td>0.211944</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.954654</td>\n",
       "      <td>24000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.275093</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.974543</td>\n",
       "      <td>24500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>0.2657</td>\n",
       "      <td>0.371518</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.994431</td>\n",
       "      <td>25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25140</td>\n",
       "      <td>0.264719</td>\n",
       "      <td>0.888576</td>\n",
       "      <td>0.646307</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.566625</td>\n",
       "      <td>138.7585</td>\n",
       "      <td>161.050</td>\n",
       "      <td>20.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>linear_probing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25140</td>\n",
       "      <td>0.264719</td>\n",
       "      <td>0.888576</td>\n",
       "      <td>0.646307</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.566625</td>\n",
       "      <td>129.1340</td>\n",
       "      <td>173.053</td>\n",
       "      <td>21.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model    loss  grad_norm  learning_rate     epoch   step  \\\n",
       "94   linear_probing  0.2665   0.252617       0.000073  0.855211  21500   \n",
       "95   linear_probing  0.2712   0.485519       0.000063  0.875099  22000   \n",
       "96   linear_probing  0.2886   0.259065       0.000053  0.894988  22500   \n",
       "97   linear_probing  0.2799   0.450012       0.000043  0.914877  23000   \n",
       "98   linear_probing  0.2872   0.483303       0.000033  0.934765  23500   \n",
       "99   linear_probing  0.2850   0.211944       0.000023  0.954654  24000   \n",
       "100  linear_probing  0.2776   0.275093       0.000013  0.974543  24500   \n",
       "101  linear_probing  0.2657   0.371518       0.000003  0.994431  25000   \n",
       "102  linear_probing     NaN        NaN            NaN  1.000000  25140   \n",
       "103  linear_probing     NaN        NaN            NaN  1.000000  25140   \n",
       "\n",
       "     eval_loss  eval_accuracy   eval_f1  eval_precision  eval_recall  \\\n",
       "94         NaN            NaN       NaN             NaN          NaN   \n",
       "95         NaN            NaN       NaN             NaN          NaN   \n",
       "96         NaN            NaN       NaN             NaN          NaN   \n",
       "97         NaN            NaN       NaN             NaN          NaN   \n",
       "98         NaN            NaN       NaN             NaN          NaN   \n",
       "99         NaN            NaN       NaN             NaN          NaN   \n",
       "100        NaN            NaN       NaN             NaN          NaN   \n",
       "101        NaN            NaN       NaN             NaN          NaN   \n",
       "102   0.264719       0.888576  0.646307        0.752066     0.566625   \n",
       "103   0.264719       0.888576  0.646307        0.752066     0.566625   \n",
       "\n",
       "     eval_runtime  eval_samples_per_second  eval_steps_per_second  \n",
       "94            NaN                      NaN                    NaN  \n",
       "95            NaN                      NaN                    NaN  \n",
       "96            NaN                      NaN                    NaN  \n",
       "97            NaN                      NaN                    NaN  \n",
       "98            NaN                      NaN                    NaN  \n",
       "99            NaN                      NaN                    NaN  \n",
       "100           NaN                      NaN                    NaN  \n",
       "101           NaN                      NaN                    NaN  \n",
       "102      138.7585                  161.050                 20.136  \n",
       "103      129.1340                  173.053                 21.636  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для интереса вытаскиваем историю лоссов/оценок\n",
    "# вытаскиваем логи конкретного тренера\n",
    "def extract_log_history(trainer, name):\n",
    "    rows = []\n",
    "    for h in trainer.state.log_history:\n",
    "        if \"loss\" in h or \"eval_loss\" in h:\n",
    "            rows.append({\"model\": name, **h})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# объединяем историю из всех запусков\n",
    "logs = pd.concat([\n",
    "    extract_log_history(full_trainer, \"full\"),\n",
    "    extract_log_history(lp_trainer, \"linear_probing\")\n",
    "    # extract_log_history(lora_trainer, \"lora\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "# смотрим хвост\n",
    "logs.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b350c51-f7a5-4078-bae4-1a88de772266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df2415-87e4-4878-87e1-57fb12ef2775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (notebook_env)",
   "language": "python",
   "name": "notebook_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
